Been experimenting with visual language models for emotion detection and the results have been pretty impressive!

Tried out LLaVa which is an end-to-end trained large multimodal model that connects a vision encoder and a LLM for general-purpose visual and language. Compared it to GPT4 vision and Llama 3.2-vision on the CK+ dataset ; the results were promising.

Using the 4-bit quantization, LLaVa achieved around 50% accuracy without fine-tuning, whereas GPT-4 outperformed with up to 90% accuracy!



LLaVA paper here : https://arxiv.org/abs/2304.08485
